{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde1786a",
   "metadata": {},
   "source": [
    "# Prueba Ingeniería de Datos - Pipeline por Microbatches\n",
    "\n",
    "Este notebook implementa un pipeline que:\n",
    "\n",
    "- Ingiera archivos CSV en **microbatches**.\n",
    "- Inserte los registros en **PostgreSQL**.\n",
    "- Mantenga estadísticas **incrementales** (`count`, `mean`, `min`, `max`) sin recalcular sobre toda la tabla.\n",
    "- Ejecute validación con `validation.csv`.\n",
    "\n",
    "**Entorno usado:**\n",
    "- Python 3\n",
    "- PostgreSQL (contenedor Docker local)\n",
    "- Librerías: pandas, SQLAlchemy, python-dotenv\n",
    "\n",
    "> Nota: Aunque los archivos CSV de este reto tienen pocas filas (máx. 40 registros),\n",
    "> el pipeline está diseñado con `chunksize` para simular un escenario de Big Data,\n",
    "> asegurando que nunca se cargan todos los archivos en memoria de manera simultánea.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f516465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test conexión con SQLAlchemy: 1\n",
      " Conexión configurada correctamente (SQLAlchemy + %sql)\n"
     ]
    }
   ],
   "source": [
    "#  Librerías principales\n",
    "import os, glob, re\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, Float, String, text, insert\n",
    "\n",
    "# Extensión SQL en notebooks\n",
    "%load_ext sql\n",
    "\n",
    "# Configuración de ipython-sql\n",
    "%config SqlMagic.autopandas = False   # False = salida en tabla (SQL puro)\n",
    "%config SqlMagic.displaycon = False   # no mostrar la URL de conexión\n",
    "\n",
    "\n",
    "#  Cargar credenciales desde .env\n",
    "load_dotenv()\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "#  Construir URL de conexión\n",
    "DB_URL = f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "#  Conectar %sql a PostgreSQL\n",
    "%sql $DB_URL\n",
    "\n",
    "#  Crear motor de conexión SQLAlchemy\n",
    "engine = create_engine(DB_URL, echo=False)\n",
    "metadata = MetaData()\n",
    "\n",
    "#  Verificar conexión\n",
    "with engine.connect() as conn:\n",
    "    test = conn.execute(text(\"SELECT 1\")).fetchone()\n",
    "    print(\"Test conexión con SQLAlchemy:\", test[0])\n",
    "\n",
    "print(\" Conexión configurada correctamente (SQLAlchemy + %sql)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff24edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas 'transactions' y 'pipeline_stats' creadas (si no existían).\n"
     ]
    }
   ],
   "source": [
    "# CELDA 3: DEFINIR MOTOR Y ESQUEMA EN LA BD\n",
    "# Definimos dos tablas:\n",
    "# 1) transactions -> guarda fila por fila los eventos\n",
    "# 2) pipeline_stats -> una fila con estadísticas acumuladas (total_count, sum_price, min, max, mean)\n",
    "\n",
    "engine = create_engine(DB_URL, echo=False)  # echo=True si quieres ver SQL generado\n",
    "metadata = MetaData()\n",
    "\n",
    "# tabla principal de transacciones\n",
    "transactions = Table(\n",
    "    \"transactions\",\n",
    "    metadata,\n",
    "    Column(\"id\", Integer, primary_key=True, autoincrement=True),\n",
    "    Column(\"timestamp\", String, nullable=False),  # guardamos como string ISO para simplicidad\n",
    "    Column(\"price\", Float, nullable=False),\n",
    "    Column(\"user_id\", String, nullable=False)\n",
    ")\n",
    "\n",
    "# tabla de estadísticas incrementales (una sola fila con id=1)\n",
    "pipeline_stats = Table(\n",
    "    \"pipeline_stats\",\n",
    "    metadata,\n",
    "    Column(\"id\", Integer, primary_key=True),\n",
    "    Column(\"total_count\", Integer, nullable=False, default=0),\n",
    "    Column(\"sum_price\", Float, nullable=False, default=0.0),\n",
    "    Column(\"min_price\", Float, nullable=True),\n",
    "    Column(\"max_price\", Float, nullable=True),\n",
    "    Column(\"mean_price\", Float, nullable=True),\n",
    "    Column(\"last_updated\", String, nullable=True)  # guardamos timestamp en texto (ISO)\n",
    ")\n",
    "\n",
    "# crear tablas si no existen\n",
    "metadata.create_all(engine)\n",
    "print(\"Tablas 'transactions' y 'pipeline_stats' creadas (si no existían).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967bd53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila inicial ya existe.\n"
     ]
    }
   ],
   "source": [
    "# CELDA 4: INICIALIZAR FILA DE STATS (id = 1)\n",
    "# Esto asegura que exista una fila con id=1 que mantendrá los acumulados.\n",
    "\n",
    "def ensure_stats_row():\n",
    "    with engine.begin() as conn:\n",
    "        res = conn.execute(text(\"SELECT id FROM pipeline_stats WHERE id = 1\")).fetchone()\n",
    "        if not res:\n",
    "            conn.execute(text(\"\"\"\n",
    "                INSERT INTO pipeline_stats (id, total_count, sum_price, min_price, max_price, mean_price, last_updated)\n",
    "                VALUES (1, 0, 0.0, NULL, NULL, NULL, now())\n",
    "            \"\"\"))\n",
    "            print(\"Fila inicial en pipeline_stats creada.\")\n",
    "        else:\n",
    "            print(\"Fila inicial ya existe.\")\n",
    "\n",
    "ensure_stats_row()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a201d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 5: FUNCIONES PARA ACTUALIZACIÓN INCREMENTAL DE STATS\n",
    "# update_stats_db actualizará la fila en pipeline_stats sumando los acumulados del \"chunk\".\n",
    "# Importante: NO hacemos SELECT AVG(...) sobre 'transactions'. Solo actualizamos la fila de stats.\n",
    "\n",
    "def update_stats_db(conn, chunk_count, chunk_sum, chunk_min, chunk_max):\n",
    "    \"\"\"\n",
    "    Recibe valores del chunk y actualiza la fila pipeline_stats (id=1) de forma incremental.\n",
    "    - conn: conexión SQLAlchemy dentro de engine.begin()\n",
    "    - chunk_count: número de filas del chunk (int)\n",
    "    - chunk_sum: suma de precios del chunk (float)\n",
    "    - chunk_min: min del chunk (float)\n",
    "    - chunk_max: max del chunk (float)\n",
    "    \"\"\"\n",
    "    # Obtener valores actuales\n",
    "    row = conn.execute(text(\"SELECT total_count, sum_price, min_price, max_price FROM pipeline_stats WHERE id=1\")).fetchone()\n",
    "    old_count = int(row[0]) if row and row[0] is not None else 0\n",
    "    old_sum = float(row[1]) if row and row[1] is not None else 0.0\n",
    "    old_min = float(row[2]) if row and row[2] is not None else float(\"inf\")\n",
    "    old_max = float(row[3]) if row and row[3] is not None else float(\"-inf\")\n",
    "\n",
    "    # Combinar acumulados\n",
    "    new_count = old_count + int(chunk_count)\n",
    "    new_sum = old_sum + float(chunk_sum)\n",
    "    new_min = min(old_min, float(chunk_min)) if chunk_min is not None else (None if old_min == float(\"inf\") else old_min)\n",
    "    new_max = max(old_max, float(chunk_max)) if chunk_max is not None else (None if old_max == float(\"-inf\") else old_max)\n",
    "    new_mean = new_sum / new_count if new_count > 0 else None\n",
    "\n",
    "    # Normalizar min/max para guardar NULL si no hay valores válidos\n",
    "    min_val = None if new_min == float(\"inf\") else new_min\n",
    "    max_val = None if new_max == float(\"-inf\") else new_max\n",
    "\n",
    "    # Actualizar la fila\n",
    "    conn.execute(text(\"\"\"\n",
    "        UPDATE pipeline_stats\n",
    "        SET total_count = :new_count,\n",
    "            sum_price = :new_sum,\n",
    "            min_price = :min_val,\n",
    "            max_price = :max_val,\n",
    "            mean_price = :new_mean,\n",
    "            last_updated = now()\n",
    "        WHERE id = 1\n",
    "    \"\"\"), {\"new_count\": new_count, \"new_sum\": new_sum, \"min_val\": min_val, \"max_val\": max_val, \"new_mean\": new_mean})\n",
    "\n",
    "    # Devolver valores para impresión / verificación\n",
    "    return {\"total_count\": new_count, \"sum_price\": new_sum, \"min_price\": min_val, \"max_price\": max_val, \"mean_price\": new_mean}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ecb205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos de entrenamiento (ordenados):\n",
      " - ../data\\2012-1.csv\n",
      " - ../data\\2012-2.csv\n",
      " - ../data\\2012-3.csv\n",
      " - ../data\\2012-4.csv\n",
      " - ../data\\2012-5.csv\n",
      "Archivo validation: ../data\\validation.csv\n"
     ]
    }
   ],
   "source": [
    "# CELDA 6: LISTAR Y ORDENAR ARCHIVOS CSV (excluir validation.csv)\n",
    "# Asume carpeta 'data/' al mismo nivel que 'notebooks/'\n",
    "\n",
    "data_folder = \"../data\"  \n",
    "if not os.path.isdir(data_folder):\n",
    "    data_folder = \"data\"  \n",
    "\n",
    "all_csv = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "\n",
    "# ordenar por número en el nombre de archivo (ej: 2012-1, 2012-2, ...)\n",
    "def sort_key(path):\n",
    "    name = os.path.basename(path)\n",
    "    nums = re.findall(r'\\d+', name)\n",
    "    return tuple(map(int, nums)) if nums else (9999,)\n",
    "\n",
    "all_csv_sorted = sorted(all_csv, key=sort_key)\n",
    "\n",
    "# separar validation.csv\n",
    "train_files = [f for f in all_csv_sorted if os.path.basename(f).lower() != \"validation.csv\"]\n",
    "validation_file = next((f for f in all_csv_sorted if os.path.basename(f).lower() == \"validation.csv\"), None)\n",
    "\n",
    "print(\"Archivos de entrenamiento (ordenados):\")\n",
    "for f in train_files:\n",
    "    print(\" -\", f)\n",
    "print(\"Archivo validation:\", validation_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934f4a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 7: FUNCIÓN PRINCIPAL PARA PROCESAR CSV POR CHUNKS\n",
    "# - chunk_size: número de filas leídas por pandas en cada iteración\n",
    "# - update_per: 'chunk' (por chunk) o 'row' (actualizar stats fila por fila; más lento)\n",
    "\n",
    "def process_csv_file(file_path, chunk_size=10, update_per='chunk'):\n",
    "    \"\"\"\n",
    "    Procesa un archivo CSV en microbatches (chunks).\n",
    "    Inserta los registros en 'transactions' y actualiza la fila 'pipeline_stats' incrementalmente.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcesando archivo: {file_path}\")\n",
    "    reader = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "    total_chunks = 0\n",
    "    for chunk in reader:\n",
    "        total_chunks += 1\n",
    "        # --- limpieza / transformaciones básicas ---\n",
    "        # parse timestamp a formato uniforme (ISO-like), convertir price a float\n",
    "        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "        chunk = chunk.dropna(subset=['timestamp', 'price', 'user_id'])  # quitar filas con nulls críticos\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # convertir timestamp a string legible para la BD (YYYY-MM-DD HH:MM:SS)\n",
    "        chunk['timestamp'] = chunk['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        chunk['price'] = pd.to_numeric(chunk['price'], errors='coerce')\n",
    "        chunk = chunk.dropna(subset=['price'])\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # preparar registros para inserción (lista de dicts)\n",
    "        records = chunk.to_dict(orient='records')\n",
    "\n",
    "        # calcular agregados del chunk (para actualizar stats)\n",
    "        chunk_count = len(records)\n",
    "        chunk_sum = float(chunk['price'].sum())\n",
    "        chunk_min = float(chunk['price'].min())\n",
    "        chunk_max = float(chunk['price'].max())\n",
    "\n",
    "        # --- inserción y actualización de stats en la misma transacción ---\n",
    "        with engine.begin() as conn:\n",
    "            # insertar registros (executemany)\n",
    "            conn.execute(transactions.insert(), records)\n",
    "\n",
    "            # actualizar stats: por chunk (recomendado) o por fila (opción)\n",
    "            if update_per == 'row':\n",
    "                # opción \"por fila\" (más costoso): actualizar por cada registro\n",
    "                for r in records:\n",
    "                    update_stats_db(conn, 1, r['price'], r['price'], r['price'])\n",
    "            else:\n",
    "                # opción eficiente: actualizar usando los agregados del chunk\n",
    "                update_stats_db(conn, chunk_count, chunk_sum, chunk_min, chunk_max)\n",
    "\n",
    "        # después de la transacción, podemos leer la fila stats para mostrar estado\n",
    "        with engine.connect() as conn2:\n",
    "            stats_row = conn2.execute(text(\"SELECT total_count, mean_price, min_price, max_price FROM pipeline_stats WHERE id=1\")).fetchone()\n",
    "            print(f\"  Chunk {total_chunks} procesado. Stats (DB): count={stats_row[0]}, mean={stats_row[1]}, min={stats_row[2]}, max={stats_row[3]}\")\n",
    "\n",
    "    print(f\"Finalizado archivo: {file_path}. Total chunks: {total_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f09f85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando archivo: ../data\\2012-1.csv\n",
      "  Chunk 1 procesado. Stats (DB): count=451, mean=57.13968957871397, min=10.0, max=100.0\n",
      "  Chunk 2 procesado. Stats (DB): count=460, mean=57.05434782608695, min=10.0, max=100.0\n",
      "  Chunk 3 procesado. Stats (DB): count=461, mean=57.12147505422993, min=10.0, max=100.0\n",
      "Finalizado archivo: ../data\\2012-1.csv. Total chunks: 3\n",
      "\n",
      "Procesando archivo: ../data\\2012-2.csv\n",
      "  Chunk 1 procesado. Stats (DB): count=471, mean=57.2208067940552, min=10.0, max=100.0\n",
      "  Chunk 2 procesado. Stats (DB): count=481, mean=56.91060291060291, min=10.0, max=100.0\n",
      "  Chunk 3 procesado. Stats (DB): count=490, mean=56.98571428571429, min=10.0, max=100.0\n",
      "Finalizado archivo: ../data\\2012-2.csv. Total chunks: 3\n",
      "\n",
      "Procesando archivo: ../data\\2012-3.csv\n",
      "  Chunk 1 procesado. Stats (DB): count=500, mean=56.94, min=10.0, max=100.0\n",
      "  Chunk 2 procesado. Stats (DB): count=510, mean=57.11960784313725, min=10.0, max=100.0\n",
      "  Chunk 3 procesado. Stats (DB): count=520, mean=57.13076923076923, min=10.0, max=100.0\n",
      "  Chunk 4 procesado. Stats (DB): count=521, mean=57.145873320537426, min=10.0, max=100.0\n",
      "Finalizado archivo: ../data\\2012-3.csv. Total chunks: 4\n",
      "\n",
      "Procesando archivo: ../data\\2012-4.csv\n",
      "  Chunk 1 procesado. Stats (DB): count=531, mean=57.08662900188324, min=10.0, max=100.0\n",
      "  Chunk 2 procesado. Stats (DB): count=540, mean=57.17407407407408, min=10.0, max=100.0\n",
      "  Chunk 3 procesado. Stats (DB): count=549, mean=57.15846994535519, min=10.0, max=100.0\n",
      "Finalizado archivo: ../data\\2012-4.csv. Total chunks: 3\n",
      "\n",
      "Procesando archivo: ../data\\2012-5.csv\n",
      "  Chunk 1 procesado. Stats (DB): count=559, mean=57.025044722719144, min=10.0, max=100.0\n",
      "  Chunk 2 procesado. Stats (DB): count=569, mean=57.0298769771529, min=10.0, max=100.0\n",
      "  Chunk 3 procesado. Stats (DB): count=579, mean=57.259067357512954, min=10.0, max=100.0\n",
      "  Chunk 4 procesado. Stats (DB): count=580, mean=57.217241379310344, min=10.0, max=100.0\n",
      "Finalizado archivo: ../data\\2012-5.csv. Total chunks: 4\n",
      "\n",
      " Carga de archivos principales finalizada.\n"
     ]
    }
   ],
   "source": [
    "# CELDA 8: EJECUTAR INGESTA SOBRE LOS 5 ARCHIVOS PRINCIPALES\n",
    "\n",
    "for f in train_files:\n",
    "    process_csv_file(f, chunk_size=10, update_per='chunk')\n",
    "\n",
    "print(\"\\n Carga de archivos principales finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02b36b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTADÍSTICAS (desde pipeline_stats)\n",
      "Total rows: 580\n",
      "Suma precios: 33186.0\n",
      "Media (mean): 57.217241379310344\n",
      "Min: 10.0\n",
      "Max: 100.0\n",
      "Última actualización (DB): 2025-10-01 06:43:58.929033+00\n"
     ]
    }
   ],
   "source": [
    "# CELDA 9: IMPRIMIR ESTADÍSTICAS ACTUALES (desde pipeline_stats)\n",
    "with engine.connect() as conn:\n",
    "    row = conn.execute(text(\"SELECT total_count, sum_price, mean_price, min_price, max_price, last_updated FROM pipeline_stats WHERE id=1\")).fetchone()\n",
    "    print(\"ESTADÍSTICAS (desde pipeline_stats)\")\n",
    "    print(\"Total rows:\", row[0])\n",
    "    print(\"Suma precios:\", row[1])\n",
    "    print(\"Media (mean):\", row[2])\n",
    "    print(\"Min:\", row[3])\n",
    "    print(\"Max:\", row[4])\n",
    "    print(\"Última actualización (DB):\", row[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b142142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando validation.csv ahora (se aplicará la misma lógica)...\n",
      "\n",
      "Procesando archivo: ../data\\validation.csv\n",
      "  Chunk 1 procesado. Stats (DB): count=588, mean=57.006802721088434, min=10.0, max=100.0\n",
      "Finalizado archivo: ../data\\validation.csv. Total chunks: 1\n",
      "Validación finalizada.\n"
     ]
    }
   ],
   "source": [
    "# CELDA 10: EJECUTAR validation.csv por el mismo pipeline y mostrar cambios\n",
    "if validation_file:\n",
    "    print(\"Ejecutando validation.csv ahora (se aplicará la misma lógica)...\")\n",
    "    process_csv_file(validation_file, chunk_size=10, update_per='chunk')\n",
    "    print(\"Validación finalizada.\")\n",
    "else:\n",
    "    print(\"No se encontró validation.csv en data/.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d2929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT en transactions: 588\n",
      "pipeline_stats.total_count: 588\n",
      "pipeline_stats.mean_price: 57.006802721088434\n",
      "pipeline_stats.min_price: 10.0\n",
      "pipeline_stats.max_price: 100.0\n",
      "\n",
      "Últimas 5 filas insertadas (ejemplo):\n",
      "(588, '2012-06-08 00:00:00', 86.0, '7')\n",
      "(587, '2012-06-07 00:00:00', 13.0, '4')\n",
      "(586, '2012-06-06 00:00:00', 62.0, '3')\n",
      "(585, '2012-06-05 00:00:00', 31.0, '10')\n",
      "(584, '2012-06-04 00:00:00', 92.0, '7')\n"
     ]
    }
   ],
   "source": [
    "# CELDA 11: CONSULTAS FINALES: comparar total en transactions vs pipeline_stats\n",
    "with engine.connect() as conn:\n",
    "    res1 = conn.execute(text(\"SELECT COUNT(*) FROM transactions\")).fetchone()\n",
    "    stats = conn.execute(text(\"SELECT total_count, mean_price, min_price, max_price FROM pipeline_stats WHERE id=1\")).fetchone()\n",
    "    print(\"COUNT en transactions:\", res1[0])\n",
    "    print(\"pipeline_stats.total_count:\", stats[0])\n",
    "    print(\"pipeline_stats.mean_price:\", stats[1])\n",
    "    print(\"pipeline_stats.min_price:\", stats[2])\n",
    "    print(\"pipeline_stats.max_price:\", stats[3])\n",
    "\n",
    "    # mostrar algunas filas ejemplo\n",
    "    sample = conn.execute(text(\"SELECT id, timestamp, price, user_id FROM transactions ORDER BY id DESC LIMIT 5\")).fetchall()\n",
    "    print(\"\\nÚltimas 5 filas insertadas (ejemplo):\")\n",
    "    for r in sample:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18b84e",
   "metadata": {},
   "source": [
    "Para mayor calidad de visualización, traemos los resultados tabulados desde la base de datos creada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cafd9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>total_rows</th>\n",
       "            <th>avg_price</th>\n",
       "            <th>min_price</th>\n",
       "            <th>max_price</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>588</td>\n",
       "            <td>57.006802721088434</td>\n",
       "            <td>10.0</td>\n",
       "            <td>100.0</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[(588, 57.006802721088434, 10.0, 100.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*) as total_rows,\n",
    "       AVG(price) as avg_price,\n",
    "       MIN(price) as min_price,\n",
    "       MAX(price) as max_price\n",
    "FROM transactions;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecfeddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 10 filas en la tabla 'transactions':\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "user_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9a5056eb-dc50-4ec2-ba58-db6e3f13986c",
       "rows": [
        [
         "0",
         "1",
         "2012-01-10 00:00:00",
         "50.0",
         "9"
        ],
        [
         "1",
         "2",
         "2012-01-11 00:00:00",
         "87.0",
         "10"
        ],
        [
         "2",
         "3",
         "2012-01-12 00:00:00",
         "64.0",
         "7"
        ],
        [
         "3",
         "4",
         "2012-01-13 00:00:00",
         "20.0",
         "10"
        ],
        [
         "4",
         "5",
         "2012-01-14 00:00:00",
         "14.0",
         "10"
        ],
        [
         "5",
         "6",
         "2012-01-15 00:00:00",
         "95.0",
         "8"
        ],
        [
         "6",
         "7",
         "2012-01-16 00:00:00",
         "95.0",
         "2"
        ],
        [
         "7",
         "8",
         "2012-01-17 00:00:00",
         "62.0",
         "4"
        ],
        [
         "8",
         "9",
         "2012-01-18 00:00:00",
         "46.0",
         "8"
        ],
        [
         "9",
         "10",
         "2012-01-19 00:00:00",
         "97.0",
         "2"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-10 00:00:00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-11 00:00:00</td>\n",
       "      <td>87.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-01-12 00:00:00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-01-13 00:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2012-01-14 00:00:00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2012-01-15 00:00:00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2012-01-16 00:00:00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2012-01-17 00:00:00</td>\n",
       "      <td>62.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2012-01-18 00:00:00</td>\n",
       "      <td>46.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2012-01-19 00:00:00</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            timestamp  price user_id\n",
       "0   1  2012-01-10 00:00:00   50.0       9\n",
       "1   2  2012-01-11 00:00:00   87.0      10\n",
       "2   3  2012-01-12 00:00:00   64.0       7\n",
       "3   4  2012-01-13 00:00:00   20.0      10\n",
       "4   5  2012-01-14 00:00:00   14.0      10\n",
       "5   6  2012-01-15 00:00:00   95.0       8\n",
       "6   7  2012-01-16 00:00:00   95.0       2\n",
       "7   8  2012-01-17 00:00:00   62.0       4\n",
       "8   9  2012-01-18 00:00:00   46.0       8\n",
       "9  10  2012-01-19 00:00:00   97.0       2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Estadísticas calculadas en SQL directamente:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_rows",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "avg_price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min_price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_price",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a71c2585-f5a5-4948-8c8b-edb4e60d9b34",
       "rows": [
        [
         "0",
         "588",
         "57.006802721088434",
         "10.0",
         "100.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_rows</th>\n",
       "      <th>avg_price</th>\n",
       "      <th>min_price</th>\n",
       "      <th>max_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>588</td>\n",
       "      <td>57.006803</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_rows  avg_price  min_price  max_price\n",
       "0         588  57.006803       10.0      100.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Ejemplo 1: traer primeras 10 filas de transactions\n",
    "df_sample = pd.read_sql(text(\"SELECT * FROM transactions LIMIT 10;\"), con=engine)\n",
    "print(\"Primeras 10 filas en la tabla 'transactions':\")\n",
    "display(df_sample)\n",
    "\n",
    "# Ejemplo 2: traer estadísticas calculadas directamente con SQL\n",
    "df_stats = pd.read_sql(text(\"\"\"\n",
    "    SELECT COUNT(*) AS total_rows,\n",
    "           AVG(price) AS avg_price,\n",
    "           MIN(price) AS min_price,\n",
    "           MAX(price) AS max_price\n",
    "    FROM transactions;\n",
    "\"\"\"), con=engine)\n",
    "print(\"\\n📊 Estadísticas calculadas en SQL directamente:\")\n",
    "display(df_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00378c21",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- El pipeline cargó los CSV por microbatches (chunks).\n",
    "- Los datos se insertaron en la tabla `transactions`.\n",
    "- Las estadísticas se actualizaron de forma incremental en la tabla `pipeline_stats`.\n",
    "- La validación con `validation.csv` mostró cómo cambian los valores globales.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
