{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde1786a",
   "metadata": {},
   "source": [
    "# Prueba Ingeniería de Datos - Pipeline por Microbatches\n",
    "### Leonardo Cortés\n",
    "\n",
    "Este notebook implementa un pipeline que:\n",
    "\n",
    "- Ingiera archivos CSV en **microbatches**.\n",
    "- Inserte los registros en **PostgreSQL**.\n",
    "- Mantenga estadísticas **incrementales** (`count`, `mean`, `min`, `max`) sin recalcular sobre toda la tabla.\n",
    "- Ejecute validación con `validation.csv`.\n",
    "\n",
    "**Entorno usado:**\n",
    "- Python 3\n",
    "- PostgreSQL (contenedor Docker local)\n",
    "- Librerías: pandas, SQLAlchemy, python-dotenv\n",
    "\n",
    "> Nota: Aunque los archivos CSV de este reto tienen pocas filas (máx. 40 registros),\n",
    "> el pipeline está diseñado con `chunksize` para simular un escenario de Big Data,\n",
    "> asegurando que nunca se cargan todos los archivos en memoria de manera simultánea.\n",
    "\n",
    "Para ver el repositorio completo con la estructura modular del pipeline puedes dirigirte al siguiente enlace:  \n",
    "[Repositorio en GitHub](https://github.com/leocortes85/Prueba_pipeline_Pragma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f516465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test conexión con SQLAlchemy: 1\n",
      " Conexión configurada correctamente (SQLAlchemy + %sql)\n"
     ]
    }
   ],
   "source": [
    "#  Librerías principales\n",
    "import os, glob, re\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, Float, String, text, insert\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "# Extensión SQL en notebooks\n",
    "%load_ext sql\n",
    "\n",
    "# Configuración de ipython-sql\n",
    "%config SqlMagic.autopandas = False   # False = salida en tabla (SQL puro)\n",
    "%config SqlMagic.displaycon = False   # no mostrar la URL de conexión\n",
    "\n",
    "\n",
    "#  Cargar credenciales desde .env\n",
    "load_dotenv()\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "#  Construir URL de conexión\n",
    "DB_URL = f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "#  Conectar %sql a PostgreSQL\n",
    "%sql $DB_URL\n",
    "\n",
    "#  Crear motor de conexión SQLAlchemy\n",
    "engine = create_engine(DB_URL, echo=False)\n",
    "metadata = MetaData()\n",
    "\n",
    "#  Verificar conexión\n",
    "with engine.connect() as conn:\n",
    "    test = conn.execute(text(\"SELECT 1\")).fetchone()\n",
    "    print(\"Test conexión con SQLAlchemy:\", test[0])\n",
    "\n",
    "print(\" Conexión configurada correctamente (SQLAlchemy + %sql)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff24edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas 'file_control', 'transactions' y 'pipeline_stats' creadas (si no existían).\n"
     ]
    }
   ],
   "source": [
    "# DEFINIR MOTOR Y ESQUEMA EN LA BD\n",
    "# Definimos dos tablas:\n",
    "# 1) transactions -> guarda fila por fila los eventos\n",
    "# 2) pipeline_stats -> una fila con estadísticas acumuladas (total_count, sum_price, min, max, mean)\n",
    "\n",
    "engine = create_engine(DB_URL, echo=False)  \n",
    "metadata = MetaData()\n",
    "\n",
    "# tabla principal de transacciones\n",
    "transactions = Table(\n",
    "    \"transactions\",\n",
    "    metadata,\n",
    "    Column(\"id\", Integer, primary_key=True, autoincrement=True),\n",
    "    Column(\"timestamp\", String, nullable=False),\n",
    "    Column(\"price\", Float, nullable=False),\n",
    "    Column(\"user_id\", String, nullable=False),\n",
    "    Column(\"unique_id\", String, unique=True, nullable=False) \n",
    ")\n",
    "\n",
    "# tabla de estadísticas incrementales (una sola fila con id=1)\n",
    "pipeline_stats = Table(\n",
    "    \"pipeline_stats\",\n",
    "    metadata,\n",
    "    Column(\"id\", Integer, primary_key=True),\n",
    "    Column(\"total_count\", Integer, nullable=False, default=0),\n",
    "    Column(\"sum_price\", Float, nullable=False, default=0.0),\n",
    "    Column(\"min_price\", Float, nullable=True),\n",
    "    Column(\"max_price\", Float, nullable=True),\n",
    "    Column(\"mean_price\", Float, nullable=True),\n",
    "    Column(\"last_updated\", String, nullable=True)  # guardamos como string para simplicidad\n",
    ")\n",
    "\n",
    "# Tabla de control de archivos procesados\n",
    "file_control = Table(\n",
    "    \"file_control\",\n",
    "    metadata,\n",
    "    Column(\"file_name\", String, primary_key=True),\n",
    "    Column(\"file_hash\", String, nullable=False),\n",
    "    Column(\"total_rows\", Integer, nullable=False),\n",
    "    Column(\"last_processed\", String, nullable=True)  # guardamos como string para simplicidad\n",
    ")\n",
    "\n",
    "\n",
    "# crear tablas si no existen\n",
    "metadata.create_all(engine)\n",
    "print(\"Tablas 'file_control', 'transactions' y 'pipeline_stats' creadas (si no existían).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30c7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIONES PARA LA TABLA \"file_control\"ArithmeticError\n",
    "\n",
    "#Función para calcular hash de archivos:\n",
    "\n",
    "def file_hash(file_path):\n",
    "    \"\"\"Devuelve hash MD5 del archivo completo\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "#Función para decidir si se procesa un archivo:\n",
    "\n",
    "def should_process_file(file_path):\n",
    "    fname = os.path.basename(file_path)\n",
    "    fhash = file_hash(file_path)\n",
    "    with engine.connect() as conn:\n",
    "        row = conn.execute(\n",
    "            text(\"SELECT file_hash, total_rows FROM file_control WHERE file_name=:fname\"),\n",
    "            {\"fname\": fname}\n",
    "        ).fetchone()\n",
    "    if row is None:\n",
    "        # archivo nunca procesado\n",
    "        return True, fhash, None\n",
    "    elif row[0] != fhash:\n",
    "        # archivo modificado\n",
    "        return True, fhash, row[1]\n",
    "    else:\n",
    "        # archivo sin cambios\n",
    "        return False, fhash, row[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967bd53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila inicial en pipeline_stats creada.\n"
     ]
    }
   ],
   "source": [
    "# INICIALIZAR FILA DE STATS (id = 1)\n",
    "# Esto asegura que exista una fila con id=1 que mantendrá los acumulados.\n",
    "\n",
    "def ensure_stats_row():\n",
    "    with engine.begin() as conn:\n",
    "        res = conn.execute(text(\"SELECT id FROM pipeline_stats WHERE id = 1\")).fetchone()\n",
    "        if not res:\n",
    "            conn.execute(text(\"\"\"\n",
    "                INSERT INTO pipeline_stats (id, total_count, sum_price, min_price, max_price, mean_price, last_updated)\n",
    "                VALUES (1, 0, 0.0, NULL, NULL, NULL, now())\n",
    "            \"\"\"))\n",
    "            print(\"Fila inicial en pipeline_stats creada.\")\n",
    "        else:\n",
    "            print(\"Fila inicial ya existe.\")\n",
    "\n",
    "ensure_stats_row()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47a201d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES PARA ACTUALIZACIÓN INCREMENTAL DE STATS\n",
    "# update_stats_db actualizará la fila en pipeline_stats sumando los acumulados del \"chunk\".\n",
    "# Importante: NO hacemos SELECT AVG(...) sobre 'transactions'. Solo actualizamos la fila de stats.\n",
    "\n",
    "def update_stats_db(conn, chunk_count, chunk_sum, chunk_min, chunk_max):\n",
    "    \"\"\"\n",
    "    Recibe valores del chunk y actualiza la fila pipeline_stats (id=1) de forma incremental.\n",
    "    - conn: conexión SQLAlchemy dentro de engine.begin()\n",
    "    - chunk_count: número de filas del chunk (int)\n",
    "    - chunk_sum: suma de precios del chunk (float)\n",
    "    - chunk_min: min del chunk (float)\n",
    "    - chunk_max: max del chunk (float)\n",
    "    \"\"\"\n",
    "    # Obtener valores actuales\n",
    "    row = conn.execute(text(\"SELECT total_count, sum_price, min_price, max_price FROM pipeline_stats WHERE id=1\")).fetchone()\n",
    "    old_count = int(row[0]) if row and row[0] is not None else 0\n",
    "    old_sum = float(row[1]) if row and row[1] is not None else 0.0\n",
    "    old_min = float(row[2]) if row and row[2] is not None else float(\"inf\")\n",
    "    old_max = float(row[3]) if row and row[3] is not None else float(\"-inf\")\n",
    "\n",
    "    # Combinar acumulados\n",
    "    new_count = old_count + int(chunk_count)\n",
    "    new_sum = old_sum + float(chunk_sum)\n",
    "    new_min = min(old_min, float(chunk_min)) if chunk_min is not None else (None if old_min == float(\"inf\") else old_min)\n",
    "    new_max = max(old_max, float(chunk_max)) if chunk_max is not None else (None if old_max == float(\"-inf\") else old_max)\n",
    "    new_mean = new_sum / new_count if new_count > 0 else None\n",
    "\n",
    "    # Normalizar min/max para guardar NULL si no hay valores válidos\n",
    "    min_val = None if new_min == float(\"inf\") else new_min\n",
    "    max_val = None if new_max == float(\"-inf\") else new_max\n",
    "\n",
    "    # Actualizar la fila\n",
    "    conn.execute(text(\"\"\"\n",
    "        UPDATE pipeline_stats\n",
    "        SET total_count = :new_count,\n",
    "            sum_price = :new_sum,\n",
    "            min_price = :min_val,\n",
    "            max_price = :max_val,\n",
    "            mean_price = :new_mean,\n",
    "            last_updated = now()\n",
    "        WHERE id = 1\n",
    "    \"\"\"), {\"new_count\": new_count, \"new_sum\": new_sum, \"min_val\": min_val, \"max_val\": max_val, \"new_mean\": new_mean})\n",
    "\n",
    "    # Devolver valores para impresión / verificación\n",
    "    return {\"total_count\": new_count, \"sum_price\": new_sum, \"min_price\": min_val, \"max_price\": max_val, \"mean_price\": new_mean}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ecb205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos de entrenamiento (ordenados):\n",
      " - ../data\\2012-1.csv\n",
      " - ../data\\2012-2.csv\n",
      " - ../data\\2012-3.csv\n",
      " - ../data\\2012-4.csv\n",
      " - ../data\\2012-5.csv\n",
      "Archivo validation: ../data\\validation.csv\n"
     ]
    }
   ],
   "source": [
    "# LISTAR Y ORDENAR ARCHIVOS CSV (excluir validation.csv)\n",
    "# Asume carpeta 'data/' al mismo nivel que 'notebooks/'\n",
    "\n",
    "data_folder = \"../data\"  \n",
    "if not os.path.isdir(data_folder):\n",
    "    data_folder = \"data\"  \n",
    "\n",
    "all_csv = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "\n",
    "# ordenar por número en el nombre de archivo (ej: 2012-1, 2012-2, ...)\n",
    "def sort_key(path):\n",
    "    name = os.path.basename(path)\n",
    "    nums = re.findall(r'\\d+', name)\n",
    "    return tuple(map(int, nums)) if nums else (9999,)\n",
    "\n",
    "all_csv_sorted = sorted(all_csv, key=sort_key)\n",
    "\n",
    "# separar validation.csv\n",
    "train_files = [f for f in all_csv_sorted if os.path.basename(f).lower() != \"validation.csv\"]\n",
    "validation_file = next((f for f in all_csv_sorted if os.path.basename(f).lower() == \"validation.csv\"), None)\n",
    "\n",
    "print(\"Archivos de entrenamiento (ordenados):\")\n",
    "for f in train_files:\n",
    "    print(\" -\", f)\n",
    "print(\"Archivo validation:\", validation_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934f4a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIÓN PRINCIPAL PARA PROCESAR CSV POR CHUNKS\n",
    "# - chunk_size: número de filas leídas por pandas en cada iteración\n",
    "# - update_per: 'chunk' (por chunk) o 'row' (actualizar stats fila por fila; más lento)\n",
    "# - Gestionar la tabla de control 'file_control' manteniendo la trazabilidad de las ejecuciones\n",
    "\n",
    "def process_csv_file(file_path, chunk_size=10, update_per='chunk'):\n",
    "    \"\"\"\n",
    "    Procesa un archivo CSV en microbatches (chunks).\n",
    "    Inserta los registros en 'transactions' y actualiza la fila 'pipeline_stats' incrementalmente.\n",
    "    Usa file_control para evitar reprocesar archivos sin cambios.\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(file_path)\n",
    "    process, fhash, prev_rows = should_process_file(file_path)\n",
    "    if not process:\n",
    "        print(f\" Archivo '{fname}' ya procesado y sin cambios. Se omite.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n Procesando archivo: {fname}\")\n",
    "    reader = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    total_chunks, total_records = 0, 0\n",
    "\n",
    "    for chunk in reader:\n",
    "        total_chunks += 1\n",
    "\n",
    "        # --- limpieza / transformaciones ---\n",
    "        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "        chunk = chunk.dropna(subset=['timestamp', 'price', 'user_id'])\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        chunk['timestamp'] = chunk['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        chunk['price'] = pd.to_numeric(chunk['price'], errors='coerce')\n",
    "        chunk = chunk.dropna(subset=['price'])\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        #  Asegurar que user_id sea string antes de armar el unique_id\n",
    "        chunk['user_id'] = chunk['user_id'].astype(str)\n",
    "\n",
    "        # Generar unique_id (ej: \"2025-09-30 10:00:00_12345\")\n",
    "        chunk['unique_id'] = chunk['timestamp'] + \"_\" + chunk['user_id']\n",
    "\n",
    "        # convertir a lista de dicts para insertar\n",
    "        records = chunk.to_dict(orient='records')\n",
    "        total_records += len(records)\n",
    "\n",
    "        # calcular stats del chunk\n",
    "        chunk_count = len(records)\n",
    "        chunk_sum = float(chunk['price'].sum())\n",
    "        chunk_min = float(chunk['price'].min())\n",
    "        chunk_max = float(chunk['price'].max())\n",
    "\n",
    "        # --- inserción y actualización ---\n",
    "        with engine.begin() as conn:\n",
    "            # UPSERT en transactions\n",
    "            for r in records:\n",
    "                stmt = insert(transactions).values(r)\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=['unique_id'],\n",
    "                    set_={\n",
    "                        \"timestamp\": stmt.excluded.timestamp,\n",
    "                        \"price\": stmt.excluded.price,\n",
    "                        \"user_id\": stmt.excluded.user_id\n",
    "                    }\n",
    "                )\n",
    "                conn.execute(stmt)\n",
    "\n",
    "            # actualización de stats\n",
    "            if update_per == 'row':\n",
    "                for r in records:\n",
    "                    update_stats_db(conn, 1, r['price'], r['price'], r['price'])\n",
    "            else:\n",
    "                update_stats_db(conn, chunk_count, chunk_sum, chunk_min, chunk_max)\n",
    "\n",
    "    # --- registrar archivo en file_control ---\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(\n",
    "            text(\"\"\"\n",
    "                INSERT INTO file_control (file_name, file_hash, total_rows, last_processed)\n",
    "                VALUES (:fname, :fhash, :total_rows, now())\n",
    "                ON CONFLICT (file_name) DO UPDATE\n",
    "                SET file_hash=:fhash,\n",
    "                    total_rows=:total_rows,\n",
    "                    last_processed=now()\n",
    "            \"\"\"),\n",
    "            {\"fname\": fname, \"fhash\": fhash, \"total_rows\": total_records}\n",
    "        )\n",
    "\n",
    "    print(f\" Finalizado archivo: {fname}. Chunks: {total_chunks}, Filas procesadas: {total_records}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f09f85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Procesando archivo: 2012-1.csv\n",
      " Finalizado archivo: 2012-1.csv. Chunks: 3, Filas procesadas: 20\n",
      "\n",
      " Procesando archivo: 2012-2.csv\n",
      " Finalizado archivo: 2012-2.csv. Chunks: 3, Filas procesadas: 29\n",
      "\n",
      " Procesando archivo: 2012-3.csv\n",
      " Finalizado archivo: 2012-3.csv. Chunks: 4, Filas procesadas: 31\n",
      "\n",
      " Procesando archivo: 2012-4.csv\n",
      " Finalizado archivo: 2012-4.csv. Chunks: 3, Filas procesadas: 28\n",
      "\n",
      " Procesando archivo: 2012-5.csv\n",
      " Finalizado archivo: 2012-5.csv. Chunks: 4, Filas procesadas: 31\n",
      "\n",
      " Procesando archivo: validation.csv\n",
      " Finalizado archivo: validation.csv. Chunks: 1, Filas procesadas: 8\n",
      "\n",
      " Carga de archivos principales finalizada.\n"
     ]
    }
   ],
   "source": [
    "# EJECUTAR INGESTA SOBRE LOS 5 ARCHIVOS PRINCIPALES\n",
    "\n",
    "\n",
    "for f in train_files:\n",
    "    process_csv_file(f, chunk_size=10, update_per='chunk')\n",
    "\n",
    "if validation_file:\n",
    "    process_csv_file(validation_file, chunk_size=10, update_per='chunk')\n",
    "\n",
    "\n",
    "print(\"\\n Carga de archivos principales finalizada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a02b36b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTADÍSTICAS (desde pipeline_stats)\n",
      "Total rows: 147\n",
      "Suma precios: 8380.0\n",
      "Media (mean): 57.006802721088434\n",
      "Min: 10.0\n",
      "Max: 100.0\n",
      "Última actualización (DB): 2025-10-01 17:52:31.990912+00\n"
     ]
    }
   ],
   "source": [
    "# IMPRIMIR ESTADÍSTICAS ACTUALES (desde pipeline_stats)\n",
    "with engine.connect() as conn:\n",
    "    row = conn.execute(text(\"SELECT total_count, sum_price, mean_price, min_price, max_price, last_updated FROM pipeline_stats WHERE id=1\")).fetchone()\n",
    "    print(\"ESTADÍSTICAS (desde pipeline_stats)\")\n",
    "    print(\"Total rows:\", row[0])\n",
    "    print(\"Suma precios:\", row[1])\n",
    "    print(\"Media (mean):\", row[2])\n",
    "    print(\"Min:\", row[3])\n",
    "    print(\"Max:\", row[4])\n",
    "    print(\"Última actualización (DB):\", row[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b142142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando validation.csv ahora (se aplicará la misma lógica)...\n",
      " Archivo 'validation.csv' ya procesado y sin cambios. Se omite.\n",
      "Validación finalizada.\n"
     ]
    }
   ],
   "source": [
    "# EJECUTAR validation.csv por el mismo pipeline y mostrar cambios\n",
    "if validation_file:\n",
    "    print(\"Ejecutando validation.csv ahora (se aplicará la misma lógica)...\")\n",
    "    process_csv_file(validation_file, chunk_size=10, update_per='chunk')\n",
    "    print(\"Validación finalizada.\")\n",
    "else:\n",
    "    print(\"No se encontró validation.csv en data/.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d2929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT en transactions: 147\n",
      "pipeline_stats.total_count: 147\n",
      "pipeline_stats.mean_price: 57.006802721088434\n",
      "pipeline_stats.min_price: 10.0\n",
      "pipeline_stats.max_price: 100.0\n",
      "\n",
      "Últimas 5 filas insertadas (ejemplo):\n",
      "(147, '2012-06-08 00:00:00', 86.0, '7')\n",
      "(146, '2012-06-07 00:00:00', 13.0, '4')\n",
      "(145, '2012-06-06 00:00:00', 62.0, '3')\n",
      "(144, '2012-06-05 00:00:00', 31.0, '10')\n",
      "(143, '2012-06-04 00:00:00', 92.0, '7')\n"
     ]
    }
   ],
   "source": [
    "# CONSULTAS FINALES: comparar total en transactions vs pipeline_stats\n",
    "with engine.connect() as conn:\n",
    "    res1 = conn.execute(text(\"SELECT COUNT(*) FROM transactions\")).fetchone()\n",
    "    stats = conn.execute(text(\"SELECT total_count, mean_price, min_price, max_price FROM pipeline_stats WHERE id=1\")).fetchone()\n",
    "    print(\"COUNT en transactions:\", res1[0])\n",
    "    print(\"pipeline_stats.total_count:\", stats[0])\n",
    "    print(\"pipeline_stats.mean_price:\", stats[1])\n",
    "    print(\"pipeline_stats.min_price:\", stats[2])\n",
    "    print(\"pipeline_stats.max_price:\", stats[3])\n",
    "\n",
    "    # mostrar algunas filas ejemplo\n",
    "    sample = conn.execute(text(\"SELECT id, timestamp, price, user_id FROM transactions ORDER BY id DESC LIMIT 5\")).fetchall()\n",
    "    print(\"\\nÚltimas 5 filas insertadas (ejemplo):\")\n",
    "    for r in sample:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18b84e",
   "metadata": {},
   "source": [
    "Para mayor calidad de visualización, traemos los resultados tabulados desde la base de datos creada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cafd9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>total_rows</th>\n",
       "            <th>avg_price</th>\n",
       "            <th>min_price</th>\n",
       "            <th>max_price</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>147</td>\n",
       "            <td>57.006802721088434</td>\n",
       "            <td>10.0</td>\n",
       "            <td>100.0</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[(147, 57.006802721088434, 10.0, 100.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*) as total_rows,\n",
    "       AVG(price) as avg_price,\n",
    "       MIN(price) as min_price,\n",
    "       MAX(price) as max_price\n",
    "FROM transactions;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecfeddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 10 filas en la tabla 'transactions':\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "user_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "unique_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "1667c72d-e23b-42f2-9ce8-80986d86f7e4",
       "rows": [
        [
         "0",
         "1",
         "2012-01-10 00:00:00",
         "50.0",
         "9",
         "2012-01-10 00:00:00_9"
        ],
        [
         "1",
         "2",
         "2012-01-11 00:00:00",
         "87.0",
         "10",
         "2012-01-11 00:00:00_10"
        ],
        [
         "2",
         "3",
         "2012-01-12 00:00:00",
         "64.0",
         "7",
         "2012-01-12 00:00:00_7"
        ],
        [
         "3",
         "4",
         "2012-01-13 00:00:00",
         "20.0",
         "10",
         "2012-01-13 00:00:00_10"
        ],
        [
         "4",
         "5",
         "2012-01-14 00:00:00",
         "14.0",
         "10",
         "2012-01-14 00:00:00_10"
        ],
        [
         "5",
         "6",
         "2012-01-15 00:00:00",
         "95.0",
         "8",
         "2012-01-15 00:00:00_8"
        ],
        [
         "6",
         "7",
         "2012-01-16 00:00:00",
         "95.0",
         "2",
         "2012-01-16 00:00:00_2"
        ],
        [
         "7",
         "8",
         "2012-01-17 00:00:00",
         "62.0",
         "4",
         "2012-01-17 00:00:00_4"
        ],
        [
         "8",
         "9",
         "2012-01-18 00:00:00",
         "46.0",
         "8",
         "2012-01-18 00:00:00_8"
        ],
        [
         "9",
         "10",
         "2012-01-19 00:00:00",
         "97.0",
         "2",
         "2012-01-19 00:00:00_2"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-10 00:00:00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2012-01-10 00:00:00_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-11 00:00:00</td>\n",
       "      <td>87.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2012-01-11 00:00:00_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-01-12 00:00:00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2012-01-12 00:00:00_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-01-13 00:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2012-01-13 00:00:00_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2012-01-14 00:00:00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2012-01-14 00:00:00_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2012-01-15 00:00:00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2012-01-15 00:00:00_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2012-01-16 00:00:00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-16 00:00:00_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2012-01-17 00:00:00</td>\n",
       "      <td>62.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-01-17 00:00:00_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2012-01-18 00:00:00</td>\n",
       "      <td>46.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2012-01-18 00:00:00_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2012-01-19 00:00:00</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-19 00:00:00_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            timestamp  price user_id               unique_id\n",
       "0   1  2012-01-10 00:00:00   50.0       9   2012-01-10 00:00:00_9\n",
       "1   2  2012-01-11 00:00:00   87.0      10  2012-01-11 00:00:00_10\n",
       "2   3  2012-01-12 00:00:00   64.0       7   2012-01-12 00:00:00_7\n",
       "3   4  2012-01-13 00:00:00   20.0      10  2012-01-13 00:00:00_10\n",
       "4   5  2012-01-14 00:00:00   14.0      10  2012-01-14 00:00:00_10\n",
       "5   6  2012-01-15 00:00:00   95.0       8   2012-01-15 00:00:00_8\n",
       "6   7  2012-01-16 00:00:00   95.0       2   2012-01-16 00:00:00_2\n",
       "7   8  2012-01-17 00:00:00   62.0       4   2012-01-17 00:00:00_4\n",
       "8   9  2012-01-18 00:00:00   46.0       8   2012-01-18 00:00:00_8\n",
       "9  10  2012-01-19 00:00:00   97.0       2   2012-01-19 00:00:00_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Estadísticas calculadas en SQL directamente:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_rows",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "avg_price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min_price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_price",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e7588ce8-057f-4070-98df-8e67c90d241d",
       "rows": [
        [
         "0",
         "147",
         "57.006802721088434",
         "10.0",
         "100.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_rows</th>\n",
       "      <th>avg_price</th>\n",
       "      <th>min_price</th>\n",
       "      <th>max_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147</td>\n",
       "      <td>57.006803</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_rows  avg_price  min_price  max_price\n",
       "0         147  57.006803       10.0      100.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Ejemplo 1: traer primeras 10 filas de transactions\n",
    "df_sample = pd.read_sql(text(\"SELECT * FROM transactions LIMIT 10;\"), con=engine)\n",
    "print(\"Primeras 10 filas en la tabla 'transactions':\")\n",
    "display(df_sample)\n",
    "\n",
    "# Ejemplo 2: traer estadísticas calculadas directamente con SQL\n",
    "df_stats = pd.read_sql(text(\"\"\"\n",
    "    SELECT COUNT(*) AS total_rows,\n",
    "           AVG(price) AS avg_price,\n",
    "           MIN(price) AS min_price,\n",
    "           MAX(price) AS max_price\n",
    "    FROM transactions;\n",
    "\"\"\"), con=engine)\n",
    "print(\"\\n Estadísticas calculadas en SQL directamente:\")\n",
    "display(df_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00378c21",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- El pipeline cargó los CSV por microbatches (chunks).\n",
    "- Los datos se insertaron en la tabla `transactions`.\n",
    "- Las estadísticas se actualizaron de forma incremental en la tabla `pipeline_stats`.\n",
    "- La validación con `validation.csv` mostró cómo cambian los valores globales.\n",
    "- La tabla `control_file` guarda la trazabilidad de los registros, evitando así que se dupliquen en caso de una actualización o reproceso.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
