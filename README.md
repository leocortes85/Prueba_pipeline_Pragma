# Prueba de Ingenier√≠a de Datos ‚Äì Pipeline por Microbatches

Este proyecto implementa un **pipeline de datos** que ingiere archivos CSV en **microbatches**, los inserta en PostgreSQL y mantiene estad√≠sticas **incrementales** en tiempo real (`count`, `mean`, `min`, `max`) sin recalcular sobre toda la tabla.

El entregable incluye:

1. Un **notebook ejecutable** (`notebooks/NB_PIPELINE_PRAGMA.ipynb`) con todo el desarrollo paso a paso.
2. Un **repositorio modular** (`src/`, `scripts/`) que simula un entorno productivo.

---

## üìÇ Estructura del proyecto

Este proyecto sigue una organizaci√≥n modular para pruebas de ingenier√≠a de datos, facilitando la lectura, pruebas y mantenimiento.

# Generando el formato correcto para el README.md

prueba-ingeniero-datos/

‚îú‚îÄ‚îÄ data/ # Archivos CSV de entrada (2012-1.csv ... validation.csv)

‚îú‚îÄ‚îÄ notebooks/

‚îÇ ‚îî‚îÄ‚îÄ pipeline.ipynb # Notebook principal para la prueba

‚îú‚îÄ‚îÄ src/ # C√≥digo modular estilo productivo

‚îÇ ‚îú‚îÄ‚îÄ **init**.py

‚îÇ ‚îú‚îÄ‚îÄ config.py # Configuraci√≥n de conexi√≥n (.env, SQLAlchemy)

‚îÇ ‚îú‚îÄ‚îÄ db.py # Definici√≥n de tablas y funciones de reset

‚îÇ ‚îú‚îÄ‚îÄ stats.py # Funciones de actualizaci√≥n de estad√≠sticas

‚îÇ ‚îî‚îÄ‚îÄ pipeline.py # L√≥gica principal de ingesta por microbatches

‚îú‚îÄ‚îÄ scripts/

‚îÇ ‚îî‚îÄ‚îÄ run_pipeline.py # Script CLI para ejecutar el pipeline completo

‚îú‚îÄ‚îÄ tests/

‚îÇ ‚îî‚îÄ‚îÄ test_stats.py # Prueba unitaria simple

‚îú‚îÄ‚îÄ .env # Variables de entorno

‚îú‚îÄ‚îÄ requirements.txt # Dependencias del proyecto

‚îî‚îÄ‚îÄ README.md # Este archivo

### Notas

- Los archivos bajo `src/` contienen la l√≥gica reusable y modular.
- `notebooks/` es solo para exploraci√≥n, documentaci√≥n de pruebas y experimentaci√≥n.
- `scripts/` permite ejecutar el pipeline completo desde la terminal sin abrir el notebook.
- `tests/` contiene pruebas unitarias b√°sicas para asegurar la integridad de funciones cr√≠ticas.
- `.env` y `requirements.txt` facilitan la configuraci√≥n del entorno y la instalaci√≥n de dependencias.

---

## ‚öôÔ∏è Requisitos

- Python 3.11+
- Docker
- PostgreSQL 15 (se recomienda usar Docker)

Instalar dependencias:

pip install -r requirements.txt

---

## Levantar PostgreSQL con Docker

docker run --name prueba-postgres \
 -e POSTGRES_USER=username \
 -e POSTGRES_PASSWORD=password \
 -e POSTGRES_DB=db_name \
 -p 5432:5432 \
 -d postgres:15

Verificar que el contenedor est√© corriendo:

docker ps

## Configuraci√≥n de variables de entorno

Copiar el archivo .env.example a .env y ajustar las credenciales:

DB_USER=username
DB_PASS=password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=db_name

---

## üìí Ejecuci√≥n del Notebook

Abrir el archivo:
notebooks/pipeline.ipynb

En el notebook encontrar√°s:

Creaci√≥n de tablas (control_files, transactions, pipeline_stats).

Ingesta de 5 archivos CSV en microbatches (chunks).

Control de trazabilidad para evitar duplicaciones en la ingesta

Actualizaci√≥n incremental de estad√≠sticas.

Validaci√≥n con validation.csv.

Consultas de verificaci√≥n tanto en Python como en SQL puro.

## Ejecuci√≥n modular (estilo productivo)

El repo incluye la l√≥gica separada en m√≥dulos (src/) y un script de orquestaci√≥n (scripts/run_pipeline.py).

Para ejecutar todo el pipeline desde consola:

python scripts/run_pipeline.py

Esto:

Crea las tablas en PostgreSQL.

Reinicia el estado de la base (TRUNCATE + fila inicial en pipeline_stats).

Procesa los 5 archivos CSV de entrenamiento.

Procesa el archivo de validaci√≥n.

Imprime estad√≠sticas actualizadas por cada chunk.

## Pruebas unitarias

El directorio tests/ contiene una prueba b√°sica (pytest) para validar la funci√≥n de actualizaci√≥n de estad√≠sticas:

pytest tests/

## Tablas utilizadas

### transactions

Almacena cada transacci√≥n fila por fila.

| Columna   | Tipo                      | Descripci√≥n                                                                                          |
| --------- | ------------------------- | ---------------------------------------------------------------------------------------------------- |
| id        | INT (PK, autoincremental) | Identificador √∫nico interno.                                                                         |
| timestamp | STRING                    | Fecha/hora del evento en formato ISO (`YYYY-MM-DD HH:MM:SS`).                                        |
| price     | FLOAT                     | Valor num√©rico de la transacci√≥n.                                                                    |
| user_id   | STRING                    | Identificador del usuario.                                                                           |
| unique_id | STRING (UNIQUE)           | Clave t√©cnica generada como `timestamp_user_id`, usada para evitar duplicados y permitir **UPSERT**. |

---

### pipeline_stats

Mantiene una sola fila con estad√≠sticas acumuladas e incrementales.

| Columna      | Tipo                | Descripci√≥n                                |
| ------------ | ------------------- | ------------------------------------------ |
| id           | INT (PK, fijo en 1) | Identificador √∫nico de la fila de stats.   |
| total_count  | INT                 | N√∫mero total de registros procesados.      |
| sum_price    | FLOAT               | Suma acumulada de todos los `price`.       |
| min_price    | FLOAT               | Precio m√≠nimo observado.                   |
| max_price    | FLOAT               | Precio m√°ximo observado.                   |
| mean_price   | FLOAT               | Promedio (`sum_price / total_count`).      |
| last_updated | STRING              | Fecha/hora ISO de la √∫ltima actualizaci√≥n. |

---

### file_control

Tabla de control de archivos procesados, utilizada para trazabilidad e idempotencia.

| Columna        | Tipo        | Descripci√≥n                                                 |
| -------------- | ----------- | ----------------------------------------------------------- |
| file_name      | STRING (PK) | Nombre del archivo procesado.                               |
| file_hash      | STRING      | Hash MD5 del contenido del archivo (para detectar cambios). |
| total_rows     | INT         | N√∫mero de filas procesadas del archivo.                     |
| last_processed | STRING      | Fecha/hora ISO de la √∫ltima vez que el archivo fue cargado. |

---

Con este esquema el pipeline asegura:

- **Escalabilidad** ‚Üí procesamiento por microbatches.
- **Idempotencia** ‚Üí `unique_id` + `UPSERT` evitan duplicados.
- **Trazabilidad** ‚Üí `file_control` registra qu√© archivos se cargaron y cu√°ndo.
- **Estad√≠sticas incrementales** ‚Üí `pipeline_stats` se actualiza sin recalcular todo desde cero.

## Consideraciones

Aunque los archivos CSV del reto tienen pocas filas (m√°x. ~40), el pipeline se dise√±√≥ con microbatches (chunksize) para simular un escenario real de Big Data, evitando cargar todo en memoria.

Se proveen dos enfoques de consulta:

Python/pandas ‚Üí integraci√≥n directa con DataFrames.

SQL puro con %sql ‚Üí consultas ligeras, sin consumir memoria adicional.

## üéØ Conclusi√≥n

Este proyecto demuestra:

Dise√±o de pipelines de ingesta por microbatches.

Uso de SQLAlchemy para modelado de tablas y transacciones.

Manejo de estad√≠sticas incrementales.

Buenas pr√°cticas de modularizaci√≥n (separaci√≥n en src/, scripts/, tests/).

Entrega en dos formatos: notebook ejecutable y repo productivo.
