# Prueba de IngenierÃ­a de Datos â€“ Pipeline por Microbatches

Este proyecto implementa un **pipeline de datos** que ingiere archivos CSV en **microbatches**, los inserta en PostgreSQL y mantiene estadÃ­sticas **incrementales** en tiempo real (`count`, `mean`, `min`, `max`) sin recalcular sobre toda la tabla.

El entregable incluye:

1. Un **notebook ejecutable** (`notebooks/NB_PIPELINE_PRAGMA.ipynb`) con todo el desarrollo paso a paso.
2. Un **repositorio modular** (`src/`, `scripts/`) que simula un entorno productivo.

---

## ğŸ“‚ Estructura del proyecto

Este proyecto sigue una organizaciÃ³n modular para pruebas de ingenierÃ­a de datos, facilitando la lectura, pruebas y mantenimiento.

# Generando el formato correcto para el README.md

prueba-ingeniero-datos/

â”œâ”€â”€ data/ # Archivos CSV de entrada (2012-1.csv ... validation.csv)

â”œâ”€â”€ notebooks/

â”‚ â””â”€â”€ pipeline.ipynb # Notebook principal para la prueba

â”œâ”€â”€ src/ # CÃ³digo modular estilo productivo

â”‚ â”œâ”€â”€ **init**.py

â”‚ â”œâ”€â”€ config.py # ConfiguraciÃ³n de conexiÃ³n (.env, SQLAlchemy)

â”‚ â”œâ”€â”€ db.py # DefiniciÃ³n de tablas y funciones de reset

â”‚ â”œâ”€â”€ stats.py # Funciones de actualizaciÃ³n de estadÃ­sticas

â”‚ â””â”€â”€ pipeline.py # LÃ³gica principal de ingesta por microbatches

â”œâ”€â”€ scripts/

â”‚ â””â”€â”€ run_pipeline.py # Script CLI para ejecutar el pipeline completo

â”œâ”€â”€ tests/

â”‚ â””â”€â”€ test_stats.py # Prueba unitaria simple

â”œâ”€â”€ .env # Variables de entorno

â”œâ”€â”€ requirements.txt # Dependencias del proyecto

â””â”€â”€ README.md # Este archivo

### Notas

- Los archivos bajo `src/` contienen la lÃ³gica reusable y modular.
- `notebooks/` es solo para exploraciÃ³n, documentaciÃ³n de pruebas y experimentaciÃ³n.
- `scripts/` permite ejecutar el pipeline completo desde la terminal sin abrir el notebook.
- `tests/` contiene pruebas unitarias bÃ¡sicas para asegurar la integridad de funciones crÃ­ticas.
- `.env` y `requirements.txt` facilitan la configuraciÃ³n del entorno y la instalaciÃ³n de dependencias.

---

## âš™ï¸ Requisitos

- Python 3.11+
- Docker
- PostgreSQL 15 (se recomienda usar Docker)

Instalar dependencias:

pip install -r requirements.txt

---

## Levantar PostgreSQL con Docker

docker run --name prueba-postgres \
 -e POSTGRES_USER=username \
 -e POSTGRES_PASSWORD=password \
 -e POSTGRES_DB=db_name \
 -p 5432:5432 \
 -d postgres:15

Verificar que el contenedor estÃ© corriendo:

docker ps

## ConfiguraciÃ³n de variables de entorno

Copiar el archivo .env.example a .env y ajustar las credenciales:

DB_USER=username
DB_PASS=password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=db_name

---

## ğŸ“’ EjecuciÃ³n del Notebook

Abrir el archivo:
notebooks/pipeline.ipynb

En el notebook encontrarÃ¡s:

CreaciÃ³n de tablas (transactions, pipeline_stats).

Ingesta de 5 archivos CSV en microbatches (chunks).

ActualizaciÃ³n incremental de estadÃ­sticas.

ValidaciÃ³n con validation.csv.

Consultas de verificaciÃ³n tanto en Python como en SQL puro.

## EjecuciÃ³n modular (estilo productivo)

El repo incluye la lÃ³gica separada en mÃ³dulos (src/) y un script de orquestaciÃ³n (scripts/run_pipeline.py).

Para ejecutar todo el pipeline desde consola:

python scripts/run_pipeline.py

Esto:

Crea las tablas en PostgreSQL.

Reinicia el estado de la base (TRUNCATE + fila inicial en pipeline_stats).

Procesa los 5 archivos CSV de entrenamiento.

Procesa el archivo de validaciÃ³n.

Imprime estadÃ­sticas actualizadas por cada chunk.

## Pruebas unitarias

El directorio tests/ contiene una prueba bÃ¡sica (pytest) para validar la funciÃ³n de actualizaciÃ³n de estadÃ­sticas:

pytest tests/

## Tablas utilizadas

### transactions

Almacena cada transacciÃ³n fila por fila.

id (PK, autoincremental)

timestamp (string en formato ISO)

price (float)

user_id (string)

### pipeline_stats

Mantiene una sola fila con estadÃ­sticas incrementales.

id (PK, fijo en 1)

total_count

sum_price

min_price

max_price

mean_price

last_updated

## Consideraciones

Aunque los archivos CSV del reto tienen pocas filas (mÃ¡x. ~40), el pipeline se diseÃ±Ã³ con microbatches (chunksize) para simular un escenario real de Big Data, evitando cargar todo en memoria.

Se proveen dos enfoques de consulta:

Python/pandas â†’ integraciÃ³n directa con DataFrames.

SQL puro con %sql â†’ consultas ligeras, sin consumir memoria adicional.

## ğŸ¯ ConclusiÃ³n

Este proyecto demuestra:

DiseÃ±o de pipelines de ingesta por microbatches.

Uso de SQLAlchemy para modelado de tablas y transacciones.

Manejo de estadÃ­sticas incrementales.

Buenas prÃ¡cticas de modularizaciÃ³n (separaciÃ³n en src/, scripts/, tests/).

Entrega en dos formatos: notebook ejecutable y repo productivo.
